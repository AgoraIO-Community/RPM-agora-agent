# ReadyPlayerMe Avatars with Real-time Voice AI and Lip Sync

# Part 1: The Journey

## Introduction: Bringing AI to Life

Imagine having a conversation with an AI assistant that doesn't just speak to you through a speaker, but has a face, expressions, and lip movements that sync perfectly with every word. Not a pre-recorded video, not a cartoon with limited animations, but a real-time 3D avatar that responds naturally as you talk.

That's exactly what we built with Agora ConvoAI and ReadyPlayer.me avatars. And in this guide, I'll take you behind the scenes to show you how we made it happen - from the initial concept to the technical implementation, complete with code examples and insights you can use to build your own.

### What You'll Learn

- How to integrate ReadyPlayer.me avatars into a Three.js/React application
- Real-time lip synchronization using WebAudio API and viseme mapping
- Facial expression control synchronized with audio playback
- Agora RTC integration for voice streaming
- ConvoAI REST API integration for AI agent management
- Performance optimization techniques for smooth 60 FPS
- The roadmap for emotion recognition and contextual expressions

## The Vision: More Than Just Voice

Voice assistants are everywhere - Siri, Alexa, Google Assistant - they're incredibly useful but lack a human touch. We wanted to create something more engaging:

- **Visual presence**: A 3D avatar you can see and connect with
- **Real-time lip sync**: Mouth movements that match the AI's speech perfectly
- **Emotional expressions**: Facial animations that convey personality
- **Natural conversations**: Powered by advanced AI with low latency
- **Human connection**: Non-verbal communication that builds trust

The result? An AI agent that feels alive.

## The Tech Stack: Choosing the Right Tools

Building a real-time conversational AI avatar required bringing together several cutting-edge technologies. Here's why we chose each one:

### 1. **Agora ConvoAI** - The Brains

Agora's ConvoAI platform provides enterprise-grade real-time voice AI with:
- **Ultra-low latency** (< 500ms end-to-end)
- **High-quality voice streaming** via RTC
- **RESTful API** for easy integration
- **Built-in conversational intelligence**
- **Global infrastructure** with 200+ data centers

**Why Agora?** We needed rock-solid real-time communication. Agora powers video calls for companies like Coursera and The Meet Group - if it can handle millions of concurrent users, it can handle our AI avatar.

### 2. **ReadyPlayer.me** - The Face

ReadyPlayer.me offers customizable 3D avatars that are:
- **Web-optimized** (lightweight GLB files, typically 2-5MB)
- **Highly detailed** with realistic facial features
- **Built with ARKit blend shapes** for facial animations
- **Animation-ready** for Mixamo and custom animations
- **Customizable** - users can create their own avatars

**Why ReadyPlayer.me?** They solved the hard problem of creating beautiful, performant avatars. Their models come with all the morph targets we need for lip sync right out of the box.

### 3. **Three.js & React Three Fiber** - The Engine

For 3D rendering in the browser:
- **Hardware-accelerated graphics** via WebGL
- **Smooth 60 FPS performance** on modern devices
- **React integration** for easy state management
- **Extensive animation controls** and utilities
- **Active community** and rich ecosystem

**Why Three.js?** It's the de facto standard for 3D on the web. React Three Fiber makes it easy to integrate with our React application while maintaining performance.

### 4. **WebAudio API** - The Synchronizer

The secret sauce for lip sync:
- **Real-time audio analysis** with FFT
- **Frequency spectrum visualization**
- **Precise timing control** (sub-millisecond accuracy)
- **Cross-browser compatibility**
- **Direct integration** with media streams

**Why WebAudio?** This is what makes real-time lip sync possible. We can analyze the audio as it plays and update the avatar's mouth instantly.

### 5. **OpenAI GPT-4** - The Personality

For natural, contextual conversations:
- **Understanding complex queries**
- **Generating human-like responses**
- **Maintaining conversation context**
- **Customizable personality** via system prompts
- **Latest language understanding** capabilities

**Why GPT-4?** Best-in-class language understanding. The AI needs to be smart enough to have natural conversations, and GPT-4 delivers.

### 6. **Azure Speech Services** - The Voice

For text-to-speech conversion:
- **Neural TTS voices** that sound natural
- **Multiple voices** and languages
- **Fast synthesis** (< 100ms)
- **SSML support** for prosody control
- **Reliable infrastructure**

**Why Azure Speech?** Their neural voices are among the most natural-sounding available, and the service is highly reliable.

---

# Part 2: Getting Started

## Quick Start Guide

### Prerequisites

- Node.js 16+ and npm
- Agora account with App ID and Token
- ConvoAI API credentials
- OpenAI API key (or compatible LLM API)
- Azure Speech Services API key (for TTS)

### Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/AgoraIO-Community/RPM-agora-agent.git
   cd RPM-agora-agent
   ```

2. **Install Dependencies**
   ```bash
   npm install
   ```

3. **Run the Development Server**
   ```bash
   npm run dev
   ```

4. **Open in Browser**
   Navigate to `http://localhost:5173` (or the port shown in terminal)

5. **Configure Settings in UI**
   - Click the settings (â˜°) button in the top-right corner
   - Enter your API credentials in the respective tabs:
     - **Agora Tab**: App ID, Token, Channel Name
     - **ConvoAI Tab**: API Base URL, API Key, Password, Agent Name, Agent UID
     - **LLM Tab**: API URL, API Key, Model, System Message, Greeting
     - **TTS Tab**: Azure Speech API Key, Region, Voice Name
     - **ASR Tab**: Language setting
   - Settings are stored in sessionStorage and will persist during your session

7. **Connect and Start Talking**
   - Click the "Connect" button to join the Agora channel
   - The AI agent will greet you and start listening
   - Speak naturally - the avatar will respond with synchronized lip movements and expressions

---

# Part 3: Technical Deep Dive

## The Challenge: Making Avatars Talk

Before we dive into the code, let's understand the core challenge. The hardest part wasn't getting the avatar to speak - it was making it look natural. Here's why:

### Problem #1: Audio Analysis

When the AI speaks, we receive an audio stream. But how do we know what sounds are being made at any given moment?

**The Solution:** We use the WebAudio API to analyze the frequency spectrum of the audio in real-time. Different sounds (phonemes) have different frequency characteristics. By analyzing these patterns, we can detect what the AI is saying.

```javascript
// Simplified example
const analyzeAudio = (audioStream) => {
  const analyzer = audioContext.createAnalyser();
  const dataArray = new Uint8Array(analyzer.frequencyBinCount);
  
  analyzer.getByteFrequencyData(dataArray);
  
  // Human speech is primarily in 85-255 Hz range
  const speechData = dataArray.slice(5, 15);
  const volume = average(speechData);
  
  return volume; // How loud the speech is
};
```

### Problem #2: Phoneme to Viseme Mapping

A "phoneme" is a unit of sound. A "viseme" is the visual mouth shape that corresponds to that sound. For example:
- "P" and "B" sounds â†’ lips pressed together
- "O" sound â†’ rounded mouth opening
- "EE" sound â†’ wide smile shape

We needed to map audio frequencies to these mouth shapes in real-time.

**The Solution:** We created a mapping system using ARKit's standard viseme blend shapes:

```javascript
const visemeMapping = {
  viseme_PP: 'p, b, m',      // Bilabial (lips together)
  viseme_FF: 'f, v',         // Labiodental (teeth on lip)
  viseme_TH: 'th',           // Dental (tongue between teeth)
  viseme_DD: 'd, t, n',      // Alveolar (tongue on ridge)
  viseme_kk: 'k, g',         // Velar (back of tongue)
  viseme_CH: 'ch, j, sh',    // Postalveolar
  viseme_SS: 's, z',         // Sibilant
  viseme_nn: 'n, ng',        // Nasal
  viseme_RR: 'r',            // Rhotic
  viseme_aa: 'ah, aa',       // Open vowel
  viseme_E: 'eh, e',         // Mid vowel
  viseme_I: 'ee, i',         // Close vowel
  viseme_O: 'oh, o',         // Back vowel
  viseme_U: 'oo, u',         // Close back vowel
};
```

### Problem #3: Smooth Animations

Simply snapping between mouth shapes would look robotic. We needed smooth, natural transitions.

**The Solution:** Linear interpolation (lerp) with easing functions:

```javascript
// Smoothly transition from current position to target
const smoothTransition = (current, target, speed = 0.3) => {
  return current + (target - current) * speed;
};

// In the animation loop (runs 60 times per second)
useFrame(() => {
  const targetMouthOpen = calculateMouthOpening();
  const currentMouthOpen = avatar.mouthOpen;
  
  avatar.mouthOpen = smoothTransition(currentMouthOpen, targetMouthOpen);
});
```

This creates buttery-smooth animations that look natural to the human eye.

## How It Works: The Complete Flow

Let me walk you through the complete flow of a conversation:

### Step 1: User Speaks
```
User: "Hey, what's the weather like today?"
      â†“
   Microphone captures audio
      â†“
   Agora RTC sends to ConvoAI
      â†“
   Speech-to-Text converts to text
```

### Step 2: AI Thinks
```
   Text sent to GPT-4
      â†“
   GPT-4 processes context & generates response
      â†“
   Response: "The weather today is sunny with a high of 75Â°F!"
```

### Step 3: AI Speaks
```
   Text sent to Azure TTS
      â†“
   TTS generates speech audio
      â†“
   Audio streamed via Agora RTC
      â†“
   User hears response
```

### Step 4: Avatar Animates (The Magic!)
```
   Audio stream arrives
      â†“
   WebAudio API analyzes frequencies
      â†“
   Detect speech patterns
      â†“
   Map to visemes (mouth shapes)
      â†“
   Update avatar morph targets
      â†“
   Render at 60 FPS
      â†“
   User sees realistic lip sync! ðŸŽ‰
```

All of this happens in real-time with less than 100ms of latency from audio to visual update.

---

## Avatar Integration Deep Dive

### Technology Stack

- **Frontend Framework**: React + Vite
- **3D Rendering**: Three.js + React Three Fiber
- **Avatar Models**: ReadyPlayer.me GLB files
- **Real-Time Communication**: Agora RTC SDK
- **AI Integration**: Agora ConvoAI REST API
- **Audio Processing**: WebAudio API
- **State Management**: React Context API
- **Styling**: Tailwind CSS

### Project Structure

```
agora-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Avatar.jsx           # 3D avatar with lip sync & expressions
â”‚   â”‚   â”œâ”€â”€ Experience.jsx       # Three.js scene setup
â”‚   â”‚   â”œâ”€â”€ UI.jsx              # Main UI controls
â”‚   â”‚   â”œâ”€â”€ Settings.jsx        # Configuration panel
â”‚   â”‚   â””â”€â”€ CombinedChat.jsx    # Chat interface
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useAgora.jsx        # Agora RTC integration
â”‚   â”‚   â”œâ”€â”€ useChat.jsx         # Chat & AI state management
â”‚   â”‚   â””â”€â”€ useLipSync.jsx      # Lip sync logic
â”‚   â”œâ”€â”€ App.jsx                 # Root component
â”‚   â””â”€â”€ main.jsx               # Entry point
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ Avatars/           # ReadyPlayer.me GLB files
â”‚   â””â”€â”€ animations/            # FBX animation files
â””â”€â”€ package.json
```

---

## Avatar Integration Deep Dive

### Why ReadyPlayer.me?

ReadyPlayer.me provides high-quality, customizable 3D avatars that are:
- **Optimized for web**: Lightweight GLB format
- **Morph target enabled**: Built-in facial blend shapes for expressions
- **Animation-ready**: Compatible with standard Mixamo animations
- **Viseme-capable**: Mouth morph targets for realistic lip sync

### Getting Your Avatar

1. **Create an Avatar**
   - Visit [readyplayer.me](https://readyplayer.me/)
   - Customize your avatar
   - Download as GLB format
   - Ensure "Use Blendshapes" option is enabled

2. **Add to Project**
   - Place GLB file in `public/models/Avatars/`
   - Name it descriptively (e.g., `Aurora.glb`, `Celeste.glb`)

3. **Load in Avatar Component**
   ```javascript
   const { scene } = useGLTF(`/models/Avatars/${currentAvatar}.glb`);
   ```

### Avatar Component Structure

The `Avatar.jsx` component handles:
- **Model Loading**: Loading GLB files with `useGLTF`
- **Animation Control**: Managing Mixamo animations with `useAnimations`
- **Morph Targets**: Controlling facial expressions via blend shapes
- **Lip Sync**: Real-time mouth movements synced to audio

---

## Real-Time Lip Sync Implementation

This is the core innovation of the project - making avatars appear to speak in real-time as the AI agent talks. Let's break down exactly how we achieved this.

### The Architecture

```
Agora Audio â†’ WebAudio Analyzer â†’ Frequency Detection â†’ Viseme Mapping â†’ Morph Targets â†’ Render (60 FPS)
```

The system analyzes audio frequencies in real-time, maps them to mouth shapes (visemes), and updates avatar blend shapes with <50ms latency.

### The Solution: WebAudio API + Viseme Mapping

#### Step 1: Audio Stream Capture

We use WebAudio API to analyze Agora's audio stream:

```javascript
const analyzer = audioContext.createAnalyser();
analyzer.fftSize = 1024;  // Frequency resolution
analyzer.smoothingTimeConstant = 0.3;  // Smooth transitions
source.connect(analyzer);
```

#### Step 2: Frequency Analysis

Analyze the speech frequency range (85-255 Hz):

```javascript
const dataArray = new Uint8Array(analyzer.frequencyBinCount);
analyzer.getByteFrequencyData(dataArray);
const speechRange = dataArray.slice(5, 15);  // 85-255 Hz range
const volume = speechRange.reduce((a, b) => a + b) / speechRange.length / 255;
```

#### Step 3: Viseme Mapping

Map audio to ARKit blend shapes:

```javascript
const visemes = {
  viseme_aa: 0.3,  // ah, aa (open)
  viseme_E: 0.2,   // eh, e (mid)
  viseme_I: 0.1,   // ee, i (close)
  viseme_O: 0.4,   // oh, o (round)
  viseme_U: 0.2,   // oo, u (tight)
  // Plus consonants: PP, FF, TH, DD, kk, CH, SS, nn, RR
};
```

See `Avatar.jsx` for the complete viseme set.

#### Step 4: Real-Time Animation

Update morph targets at 60 FPS:

```javascript
useFrame(() => {
  const volume = getAudioLevel();
  const targetValue = volume * visemeWeight;
  const current = morphTargetInfluences[visemeIndex];
  
  // Smooth interpolation (lerp)
  morphTargetInfluences[visemeIndex] = lerp(current, targetValue, 0.3);
});
```

For the complete implementation, see `src/hooks/useLipSync.jsx` in the repository.

### Integration with Agora

In `useAgora.jsx`, we connect the lip sync when remote audio arrives:

```javascript
rtc.client.on("user-published", async (user, mediaType) => {
  if (mediaType === "audio") {
    await rtc.client.subscribe(user, "audio");
    const remoteTrack = user.audioTrack;
    
    // Setup lip sync with remote audio
    setupLipSync(remoteTrack.getMediaStreamTrack());
    
    remoteTrack.play();
  }
});
```

The complete implementation in `src/components/Avatar.jsx` analyzes audio frequencies, maps them to visemes based on frequency patterns (low freq = O/U sounds, high freq = I/E sounds, mid freq = A sounds), and smoothly updates morph targets at 60 FPS.

---

## Facial Expressions System

Beyond lip sync, we control broader facial expressions to convey emotion and engagement.

### Expression System

We've implemented 7 expressions (default, smile, surprised, funny, sad, angry, crazy), each combining multiple ARKit blend shapes:

```javascript
const expressions = {
  smile: { mouthSmile: 0.8, eyeSquintLeft: 0.2, eyeSquintRight: 0.2 },
  surprised: { browInnerUp: 0.9, eyeWideLeft: 0.8, jawOpen: 0.3 },
  // See Avatar.jsx for all expressions
};
```

Expressions transition smoothly over 300ms using easing functions:

```javascript
const setExpression = (name) => {
  const target = expressions[name];
  Object.entries(target).forEach(([key, value]) => {
    animateMorphTarget(key, value, 300); // Smooth 300ms transition
  });
};
```

Lip sync overlays on expressions using additive blending:

```javascript
// Apply lip sync ON TOP of expression using Math.max()
morphTargets[viseme] = Math.max(expressionValue, lipSyncValue);
```

**Important:** We use `Math.max()` to ensure lip sync movements are additive and don't reduce expression intensity.

---

## Performance Optimization

Running real-time 3D graphics, audio analysis, and AI inference simultaneously is demanding. Here's how we keep it smooth at 60 FPS:

### Key Optimizations

1. **Skip silent frames**: Only analyze when volume > 0.01
2. **Throttle updates**: Update morph targets at 30 FPS (visually smooth)
3. **Cache avatars**: Load GLB files once, reuse from Map
4. **Delta checking**: Skip render if values haven't changed
5. **Cleanup**: Dispose geometry/materials on unmount

### Performance Metrics

On a modern device (M1 MacBook or equivalent), we achieve:
- **60 FPS** render rate
- **< 50ms** audio-to-visual latency
- **< 5% CPU** usage for lip sync
- **< 100MB** memory for single avatar

---

# Part 4: Future & Beyond

## What's Next: Emotion Recognition

The current system works great, but we're taking it further. In the next version, we're adding **automatic emotion detection** to match avatar expressions to the AI's emotional tone.

### The Vision

Imagine the AI saying:
- *"I'm so excited to help you with that!"* â†’ Avatar shows excitement with wide smile and raised eyebrows
- *"I'm sorry to hear that happened."* â†’ Avatar shows empathy with concerned expression and soft eyes
- *"Wow, that's surprising!"* â†’ Avatar shows surprise with raised eyebrows and open mouth
- *"Let me think about that..."* â†’ Avatar shows contemplation with slight squint and tilted head

### How It Will Work

#### 1. Sentiment Analysis Integration

Analyze AI responses for emotional content:

```javascript
// Planned implementation
const analyzeEmotion = async (text) => {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: 'gpt-4',
      messages: [{
        role: 'system',
        content: 'Analyze the emotion of this text and return one of: happy, sad, angry, surprised, neutral, excited'
      }, {
        role: 'user',
        content: text
      }],
      temperature: 0.3
    })
  });
  
  const data = await response.json();
  return data.choices[0].message.content.toLowerCase();
};
```

#### 2. Emotion-to-Expression Mapping

Map detected emotions to facial expressions:

```javascript
const emotionToExpression = {
  happy: 'smile',
  excited: 'smile',
  sad: 'sad',
  angry: 'angry',
  surprised: 'surprised',
  neutral: 'default',
  confused: 'funnyFace'
};
```

#### 3. Context-Aware Expression Timing

Synchronize expression changes with speech prosody:

```javascript
const scheduleExpressionChanges = (text, emotion) => {
  // Parse text for emotional markers
  const markers = detectEmotionalMarkers(text);
  
  // Calculate timing based on TTS duration
  const duration = estimateSpeechDuration(text);
  
  // Schedule expression changes
  markers.forEach((marker, index) => {
    const timing = (marker.position / text.length) * duration;
    setTimeout(() => {
      setFacialExpression(emotionToExpression[marker.emotion]);
    }, timing);
  });
};
```

#### Implementation Plan

1. **Text Analysis**: Use GPT-4 to detect emotion from AI responses
2. **Voice Analysis**: Analyze pitch, energy, and speaking rate from audio
3. **Multi-Modal Fusion**: Combine text (30%) and voice (70%) for accuracy
4. **Timing**: Parse text for emotional keywords and schedule expression changes

```javascript
// Example: Detect emotion from text and audio
const emotion = await detectEmotion(aiResponse, audioStream);
setExpression(emotionToExpression[emotion]);
```

### Other Planned Features

- **Gesture System**: Hand and body gestures synchronized with speech
- **Eye Tracking**: Realistic eye movements and gaze direction following user
- **Breathing Animation**: Subtle chest movements for lifelike presence
- **Environmental Reactions**: Avatar responds to user actions and environment
- **Custom Avatar Upload**: Allow users to use their own ReadyPlayer.me avatars
- **Multi-Language Support**: Viseme mapping optimized for different languages
- **Performance Optimization**: LOD system for mobile devices
- **Voice Cloning**: Match avatar voice to appearance
- **Multiplayer Support**: Multiple avatars in same scene

### Expected Benefits

- **More engaging**: Avatars that show emotion feel more human
- **Better communication**: Visual cues enhance understanding
- **Increased empathy**: Users connect better with expressive avatars
- **Contextual awareness**: Expressions match conversation tone
- **Improved retention**: Emotional engagement leads to better memory

---

## Real-World Applications

### Use Cases

- **Customer Service**: Empathetic avatars that show concern, celebrate solutions
- **Education**: Virtual tutors with encouraging expressions and natural reactions
- **Healthcare**: Telehealth assistants providing visual feedback and empathy
- **Entertainment**: Storytellers and game NPCs with dynamic expressions
- **Accessibility**: Language learning, speech therapy, social skills training

### The Impact

Adding visual, expressive avatars to voice AI:
- **Increases engagement** by 3-4x compared to voice-only
- **Improves comprehension** through visual cues
- **Builds trust** through facial expressions
- **Enhances accessibility** for diverse users
- **Creates emotional connection** beyond transactional interactions

---

## Troubleshooting

### Common Issues and Solutions

#### 1. Avatar Not Loading

**Symptoms:** Black screen or missing avatar

**Solutions:**
- Check GLB file is in `public/models/Avatars/`
- Verify file name matches `currentAvatar` prop
- Check browser console for loading errors
- Ensure file is valid GLB format

```javascript
// Debug loading
const { scene, error } = useGLTF(`/models/Avatars/${currentAvatar}.glb`, true);
console.log('Loading status:', error ? 'Failed' : 'Success');
```

#### 2. Lip Sync Not Working

**Symptoms:** Avatar mouth doesn't move with speech

**Solutions:**
- Check WebAudio API support: `console.log('WebAudio:', !!window.AudioContext)`
- Verify remote audio track is playing
- Check morph target names match ARKit standard
- Increase analyzer sensitivity

```javascript
// Debug audio analysis
const level = getAudioLevel();
console.log('Audio level:', level);
// Should show values > 0 when agent is speaking
```

#### 3. Expressions Not Changing

**Symptoms:** Avatar stuck in one expression

**Solutions:**
- Verify morph targets exist: `console.log(nodes.Wolf3D_Head.morphTargetDictionary)`
- Check expression name is valid
- Ensure smooth transitions aren't too slow
- Clear expression cache

```javascript
// Force reset
setFacialExpression('default');
setTimeout(() => setFacialExpression('smile'), 100);
```

#### 4. Performance Issues

**Symptoms:** Low FPS, stuttering

**Solutions:**
- Reduce `fftSize` in audio analyzer (512 instead of 1024)
- Decrease morph target update frequency
- Use simpler avatar models
- Enable hardware acceleration in browser

```javascript
// Optimize render loop
let lastUpdate = 0;
useFrame((state) => {
  const now = state.clock.getElapsedTime();
  if (now - lastUpdate < 0.016) return; // Cap at 60 FPS
  lastUpdate = now;
  
  // Update logic here
});
```

#### 5. Agora Connection Fails

**Symptoms:** Cannot connect to channel

**Solutions:**
- Verify App ID and Token are correct
- Check token hasn't expired (generate new one)
- Ensure channel name matches configuration
- Check network/firewall settings

```javascript
// Enable Agora SDK debug logs
AgoraRTC.setLogLevel(0); // 0 = DEBUG
```

#### 6. Missing Morph Targets

**Symptoms:** Some expressions don't work

**Solutions:**
- Re-export avatar from ReadyPlayer.me with blendshapes enabled
- Use official ReadyPlayer.me models (not custom GLB)
- Check morph target dictionary: `console.log(model.morphTargetDictionary)`

Expected morph targets for full compatibility:
```
browInnerUp, browOuterUpLeft, browOuterUpRight,
eyeWideLeft, eyeWideRight, eyeSquintLeft, eyeSquintRight,
mouthSmile, mouthFrown, mouthOpen, jawOpen,
viseme_aa, viseme_E, viseme_I, viseme_O, viseme_U,
viseme_PP, viseme_FF, viseme_TH, viseme_DD, viseme_kk,
viseme_CH, viseme_SS, viseme_nn, viseme_RR
```

### Key Takeaways

**Technical**: WebAudio varies by browser (test widely), ARKit blend shapes are reliable, smooth animations beat pixel-perfect accuracy, performance is non-negotiable.

**Design**: Subtle expressions look professional, smooth transitions matter more than positions, default to slight smile, always test with real users.

---

## Performance Optimization Tips

### 1. Audio Analysis Optimization

```javascript
// Use a larger FFT size for better quality (but slower)
analyzer.fftSize = 2048; // High quality, lower FPS

// Use smaller FFT for better performance
analyzer.fftSize = 512; // Lower quality, higher FPS

// Balance between quality and performance
analyzer.fftSize = 1024; // Recommended
```

### 2. Morph Target Updates

```javascript
// Only update morph targets when audio is active
useFrame(() => {
  const volume = getAudioLevel();
  if (volume < 0.01) return; // Skip if silent
  
  // Update morph targets
});
```

### 3. Expression Caching

```javascript
const expressionCache = useRef({});

const setFacialExpression = (expression) => {
  // Return early if same expression
  if (expressionCache.current === expression) return;
  expressionCache.current = expression;
  
  // Apply expression
};
```

### 4. Reduce Render Calls

```javascript
// Throttle updates to 30 FPS for better performance
const FRAME_INTERVAL = 1000 / 30;
let lastFrameTime = 0;

useFrame((state) => {
  const currentTime = state.clock.getElapsedTime() * 1000;
  if (currentTime - lastFrameTime < FRAME_INTERVAL) return;
  lastFrameTime = currentTime;
  
  // Your update logic
});
```

---

## Advanced Customization

### Custom Viseme Mapping

Create your own viseme-to-phoneme mapping for different languages:

```javascript
// English (default)
const visemeMapping = {
  viseme_aa: ['ah', 'aa', 'ae'],
  viseme_E: ['eh', 'e'],
  viseme_I: ['ee', 'i', 'y'],
  viseme_O: ['oh', 'o', 'aw'],
  viseme_U: ['oo', 'u', 'w']
};

// Spanish
const spanishVisemeMapping = {
  viseme_aa: ['a'],
  viseme_E: ['e'],
  viseme_I: ['i'],
  viseme_O: ['o'],
  viseme_U: ['u']
};
```

### Custom Expression Presets

Define your own expression combinations:

```javascript
const customExpressions = {
  thinking: {
    browInnerUp: 0.4,
    browOuterUpLeft: 0.2,
    eyeSquintLeft: 0.3,
    eyeSquintRight: 0.3,
    mouthLeft: 0.2
  },
  skeptical: {
    browInnerUp: 0.6,
    browOuterUpLeft: 0.3,
    browOuterUpRight: 0.1,
    eyeSquintLeft: 0.2,
    mouthSmileLeft: 0.1,
    mouthFrownRight: 0.1
  }
};
```

---

## Contributing & Resources

### Try It Yourself!

Want to build your own AI avatar? Here's how to get started:

#### Quick Setup (5 minutes)

1. **Clone the repo**
   ```bash
   git clone https://github.com/AgoraIO-Community/RPM-agora-agent.git
   cd RPM-agora-agent
   npm install
   ```

2. **Get your API keys**
   - Agora: [console.agora.io](https://console.agora.io)
   - OpenAI: [platform.openai.com](https://platform.openai.com)
   - Azure Speech: [portal.azure.com](https://portal.azure.com)

3. **Configure and run**
   ```bash
   npm run dev
   ```

4. **Start talking!**
   Open `http://localhost:5173`, add your credentials in settings, and start chatting with your AI avatar.

#### Customization Ideas

- **Change the avatar**: Use any ReadyPlayer.me character
- **Adjust personality**: Modify the GPT-4 system prompt
- **Add gestures**: Extend the animation system with hand movements
- **Custom expressions**: Create your own emotional presets
- **Different voices**: Try various Azure TTS voices and accents
- **Multi-language**: Add support for other languages with adjusted viseme mapping

### Contributing

We welcome contributions! Here's how you can help:

1. **Bug Reports**: Open an issue with detailed reproduction steps
2. **Feature Requests**: Describe the feature and use case in discussions
3. **Pull Requests**: Fork, create a feature branch, and submit PR
4. **Documentation**: Improve this guide or add examples and tutorials

#### Development Workflow

```bash
# Create feature branch
git checkout -b feature/your-feature-name

# Make changes and test
npm run dev

# Commit with descriptive message
git commit -m "feat: add emotion detection support"

# Push and create PR
git push origin feature/your-feature-name
```

---

## Resources & Community

### Official Documentation
- [Agora RTC SDK](https://docs.agora.io/en/voice-calling/overview/product-overview)
- [Agora ConvoAI](https://docs.agora.io/en/conversational-ai/overview/product-overview)
- [ReadyPlayer.me Documentation](https://docs.readyplayer.me/)
- [Three.js Docs](https://threejs.org/docs/)
- [React Three Fiber](https://docs.pmnd.rs/react-three-fiber/)
- [WebAudio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)

### Community & Support
- [Agora Developer Community](https://www.agora.io/en/community/)
- [Three.js Discord](https://discord.gg/three-js)
- [ReadyPlayer.me Discord](https://discord.gg/readyplayerme)
- ðŸ“§ Email: support@yourproject.com
- ðŸ’¬ Discord: [Join our community](https://discord.gg/yourproject)
- ðŸ› Issues: [GitHub Issues](https://github.com/AgoraIO-Community/RPM-agora-agent/issues)

### Related Projects & Tools
- [Mixamo Animations](https://www.mixamo.com/) - Free character animations
- [ARKit Face Tracking](https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation) - Blend shape reference
- [Oculus Lipsync](https://developer.oculus.com/documentation/unity/audio-ovrlipsync-unity/) - Alternative lip sync approach
- [Blender](https://www.blender.org/) - 3D modeling and animation

### Acknowledgments

Special thanks to:
- **Agora** for their amazing ConvoAI platform and support
- **ReadyPlayer.me** for making high-quality avatars accessible to everyone
- **Three.js community** for endless support and incredible tools
- **Open source contributors** who make projects like this possible

---

## Final Thoughts

Building real-time AI avatars was challenging but incredibly rewarding. We combined cutting-edge AI, 3D graphics, audio processing, and real-time communication to create something that feels magical.

The technology is here. The tools are available. The only limit is our imagination.

### What We Built
- âœ… Real-time conversational AI with natural dialogue
- âœ… Lifelike 3D avatars with facial expressions
- âœ… Synchronized lip movements using WebAudio analysis
- âœ… Smooth 60 FPS performance on modern devices
- âœ… Easy-to-use interface with comprehensive settings
- âœ… Open source for the community to build upon

### What's Coming Next
- ðŸš€ Automatic emotion detection and expression matching
- ðŸš€ Voice tone analysis for enhanced realism
- ðŸš€ Gesture system for natural body language
- ðŸš€ Multi-language support with optimized visemes
- ðŸš€ Mobile optimization and performance improvements

**What will you build?**

---

## License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Built with â¤ï¸ by the Agora Agent Team**  
*Last updated: December 11, 2025*

---

*If you found this guide helpful, please â­ star the repo and share it with others who might be interested in building real-time AI avatars! Together, we're building the future of human-AI interaction.*
